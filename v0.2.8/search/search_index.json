{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Labtasker","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Labtasker is an easy-to-use task queue tool designed to manage and dispatch lab experiment tasks to user-defined workers.</p> <p>What actually is labtasker? When to use? What does it do?</p> <p>Feeling confused? Here is a quick takeaway:</p> <p>TLDR: Replace <code>for</code> loops in your experiment wrapper script with labtasker to unlock a variety of powerful features (1) effortlessly.</p> <p></p> <ol> <li>Labtasker provides advanced features with only 1 extra line of code:<ul> <li>Load balancing and script parallelism</li> <li>Dynamic task prioritization</li> <li>Dynamic task cancellation</li> <li>Failure auto-retry and worker suspension</li> <li>Metadata recording</li> <li>Event notification</li> <li>And much more!</li> </ul> </li> </ol> <p>Integrating Labtasker into your existing experiment workflow requires just a few lines of boilerplate code.</p> <p>To get started, check out the quick Tutorial for an overview of the basic workflow.</p> <p>To get an overview of the motivation of this tool, continue reading.</p>"},{"location":"#motivation","title":"Motivation","text":""},{"location":"#why-not-simple-bash-wrapper-scripts","title":"Why not simple bash wrapper scripts?","text":"<p>Running multiple lab experiments on a multiple GPUs (such as most AIGC experiments) can be tedious and inefficient. Traditional scripts require manual splitting and offer limited control. Labtasker simplifies this process by allowing you to submit tasks to a server-based queue, which workers can fetch and execute.</p> <p>Below is an example of how traditional wrapper scripts scale poorly.</p> <p>Imagine you have multiple lab experiment jobs to run on a single GPU, such as for tasks like prompt engineering or hyperparameter search.</p> <p>The simplest approach is to write a script for each experiment and execute them sequentially.</p> run_job.sh<pre><code>#!/bin/bash\n\nfor arg1 in 1 2 3 4; do\n    for arg2 in 1 2 3 4; do\n        for arg3 in 1 2 3 4; do\n            python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n        done\n    done\ndone\n</code></pre> <p>This method works, but what if you have more than one worker/GPU?</p> <p>Let's say you have 4 GPUs. You would probably split the experiments into 4 groups and run them in parallel to make better use of the resources.</p> <p>This is already getting messy! \ud83d\ude30</p> run_job_1.sh<pre><code>#!/bin/bash\n\narg1=1\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_2.sh<pre><code>#!/bin/bash\n\narg1=2\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_3.sh<pre><code>#!/bin/bash\n\narg1=3\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> run_job_4.sh<pre><code>#!/bin/bash\n\narg1=4\nfor arg2 in 1 2 3 4; do\n    for arg3 in 1 2 3 4; do\n        python job_main.py --arg1 $arg1 --arg2 $arg2 --arg3 $arg3\n    done\ndone\n</code></pre> <p>However, this method can quickly become tedious and offers limited control over the experiments once the job scripts are running. Consider the following scenarios:</p> <ul> <li>How do you handle cases where the parameters are hard to divide evenly (e.g., 5x5x5 split across 3 GPUs), making it difficult to distribute the workload fairly?</li> <li>What if your script crashes halfway and you have no idea of which experiments are complete?</li> <li>What if you realize some scheduled experiments are unnecessary after reviewing the results? (Stopping the script isn't ideal, as it would kill running jobs and make it hard to track which experiments are complete.)</li> <li>What if you want to reprioritize certain experiments based on initial results? You\u2019d face the same issue as above.</li> <li>How do you append extra experiment groups during script execution?</li> <li>What if some experiments fail midway? It can be challenging to untangle nested loops and identify completed tasks.</li> </ul> <p>Labtasker is designed to tackle these challenges elegantly, with minimal disruption to your existing workflow.</p> <p>With Labtasker, you can submit a variety of experiment arguments to a server-based task queue. Worker nodes can then fetch and execute these tasks directly from the queue.</p>"},{"location":"#why-not-slurm","title":"Why not SLURM?","text":"<p>Unlike traditional HPC resource management systems like SLURM, Labtasker is tailored for users rather than system administrators.</p> <p>Labtasker is designed to be a simple and easy-to-use.</p> <ul> <li>It disentangles task queue from resource management.</li> <li>It offers a versatile task queue system that can be used by anyone (not just system administrators), without the need   for extensive configuration or knowledge of HPC systems.</li> </ul> <p>Here's are key conceptual differences between Labtasker and SLURM:</p> Aspects SLURM Labtasker Purpose HPC resource management system Task queue system for lab experiments Who is it for Designed for system administrators Designed for users Configuration Requires extensive configuration Minimal configuration needed Task Submission Jobs submitted as scripts with resource requirements Tasks submitted as argument groups (pythonic dictionaries) Resource Handling Allocates resources and runs the job Does not explicitly handle resource allocation Flexibility Assumes specific resource and task types No assumptions about task nature, experiment type, or computation resources Execution Runs jobs on allocated resources User-defined worker scripts run on various machines/GPUs/CPUs and decide how to handle the arguments Reporting Handled by the framework Reports results back to the server via API"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#installation","title":"Installation","text":""},{"location":"faq/#do-i-need-docker-compose-to-use-labtasker","title":"Do I need <code>docker compose</code> to use Labtasker?","text":"<p>No. Depends on your needs:</p> <ol> <li>If you want the simplest setup, you can use <code>labtasker-server serve</code> to run the server out-of-the-box. All you only need is a Python environment.</li> <li>If you need more scalable and robust setup, use the <code>docker compose</code> method to deploy the server.</li> <li>If you have the mongod installed locally, you can specify in the configuration file and choose <code>--db-mode external</code> when you run <code>labtasker-server serve</code>.</li> </ol>"},{"location":"faq/#do-i-need-to-deploy-the-labtasker-server","title":"Do I need to deploy the Labtasker server?","text":"<p>It depends:</p> <ol> <li>If you\u2019re using Labtasker on your own, you need to deploy the Labtasker server for the client tools to work. It is very convenient: simply run <code>labtasker-server serve</code> in the background and you\u2019re good to go.</li> <li>If you\u2019re working with others or trust someone who has already deployed the Labtasker server, you don\u2019t need to\u2014just connect to their server. (Make sure you trust the provider, as your task information is not encrypted.)</li> </ol>"},{"location":"faq/#do-i-need-to-deploy-labtasker-server-on-the-cloud-with-a-domain-name","title":"Do I need to deploy Labtasker server on the cloud? With a domain name?","text":"<p>No and no. You can run Labtasker server at localhost (could be your GPU server or your laptop). The only requirement is that the workers can access the server.</p>"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#how-to-run-job-command-with-environment-variables-using-labtasker-loop","title":"How to run job command with environment variables using <code>labtasker loop</code>?","text":"<p>\u274c The wrong way:</p> <pre><code>labtasker loop -- CUDA_VISIBLE_DEVICES=0 python '%(script)' --dataset '%(dataset)'\n</code></pre> <p>\u2705 The right way:</p> <pre><code>CUDA_VISIBLE_DEVICES=0 labtasker loop -- python '%(script)' --dataset '%(dataset)'\n</code></pre> <p>or else, you may get an error like this:</p> <pre><code>FileNotFoundError: [Errno 2] No such file or directory: 'CUDA_VISIBLE_DEVICES=0'\n</code></pre>"},{"location":"faq/#what-does-the-traceback-hook-option-do-in-labtasker-init","title":"What does the \"traceback hook\" option do in <code>labtasker init</code>?","text":"<p>The \"traceback hook\" replaces Python\u2019s default exception handler (<code>sys.excepthook</code>) to format error messages and hide sensitive information like passwords.</p> <p>If you disable this option, Labtasker won\u2019t modify <code>sys.excepthook</code>, which can help avoid conflicts with other libraries that also use it.</p>"},{"location":"develop/database/","title":"Database","text":"<p>Each queue identified by a unique queue_name, is responsible for managing:</p> <ol> <li>A collection of tasks (task queue)</li> <li>A collection of workers to check worker status. If a worker crashes multiple times, the tasks will be no longer be assigned to it. (worker pool)</li> <li>Authentication for the queue</li> </ol>"},{"location":"develop/database/#priority","title":"Priority","text":"<ul> <li>LOW: 0</li> <li>MEDIUM: 10  (default)</li> <li>HIGH: 20</li> </ul>"},{"location":"develop/database/#worker-fsm","title":"Worker FSM","text":"<p>states:</p> <ul> <li>active</li> <li>suspended</li> <li>crashed</li> </ul>"},{"location":"develop/database/#task-fsm","title":"Task FSM","text":"<p>states:</p> <ul> <li>created</li> <li>cancelled</li> <li>pending</li> <li>running</li> <li>success</li> <li>failed</li> </ul>"},{"location":"develop/database/#collections","title":"Collections","text":""},{"location":"develop/database/#queues-collection","title":"Queues Collection","text":"<pre><code>{\n    \"_id\": \"uuid-string\",\n    \"queue_name\": \"my_queue\",\n    \"password\": \"hashed_password\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"metadata\": {}\n}\n</code></pre>"},{"location":"develop/database/#tasks-collection","title":"Tasks Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"created\",\n    \"task_name\": \"optional_task_name\",\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"start_time\": \"2025-01-01T00:00:00Z\",\n    \"last_heartbeat\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\",\n    \"heartbeat_timeout\": 60,\n    \"task_timeout\": 3600,\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"priority\": 10,\n    \"metadata\": {},\n    \"args\": {\n        \"my_param_1\": 1,\n        \"my_param_2\": 2\n    },\n    \"cmd\": \"python main.py --arg1=1 --arg2=2\",\n    \"summary\": {},\n    \"worker_id\": \"xxxxxx\",\n}\n</code></pre>"},{"location":"develop/database/#workers-collection","title":"Workers Collection","text":"<pre><code>{\n    \"_id\": \"xxxxxx\",\n    \"queue_id\": \"uuid-string\",\n    \"status\": \"active\",\n    \"worker_name\": \"optional_worker_name\",\n    \"metadata\": {},\n    \"max_retries\": 3,\n    \"retries\": 0,\n    \"created_at\": \"2025-01-01T00:00:00Z\",\n    \"last_modified\": \"2025-01-01T00:00:00Z\"\n}\n</code></pre>"},{"location":"develop/development/","title":"Development Guide","text":""},{"location":"develop/development/#development-setup","title":"Development Setup","text":""},{"location":"develop/development/#pre-commit-hooks","title":"Pre-commit hooks","text":"<pre><code># Install pre-commit hooks for code formatting\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"develop/development/#install-development-dependencies","title":"Install development dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"develop/development/#development-utilities","title":"Development utilities","text":""},{"location":"develop/development/#format-code","title":"Format code","text":"<pre><code>make format\n</code></pre>"},{"location":"develop/development/#run-linters","title":"Run linters","text":"<pre><code>make lint\n</code></pre>"},{"location":"develop/development/#tests","title":"Tests","text":""},{"location":"develop/development/#test-setups","title":"Test setups","text":"<p>Tests are divided into unit tests, integration tests, and end-to-end tests. Some test cases are shared between unit tests, integration tests and end-to-end tests.</p> <p>Test settings</p> <p>Testcases are marked with <code>pytest.mark.unit</code>, <code>pytest.mark.integration</code>, and <code>pytest.mark.e2e</code>.</p> <p>Different tests adopts the following setting:</p> Test type Database Server &amp; Client Unit tests MongoMock TestClient &amp; ASGITransport, patched httpx client Integration tests docker mongodb service TestClient &amp; ASGITransport, patched httpx client End-to-end tests docker mongodb service docker fastapi service, httpx client to localhost"},{"location":"develop/development/#run-tests","title":"Run tests","text":"<p>Unit tests:</p> <pre><code>make unit-test\n</code></pre> <p>Do not run integration and e2e tests in production env</p> <p>Do not run integration and e2e tests in production env, as they will erase the database for testing.</p> <p>Integration tests:</p> <pre><code>make integration-test\n</code></pre> <p>End-to-end tests (quite time-consuming):</p> <pre><code>make e2e-test\n</code></pre>"},{"location":"develop/documentation/","title":"Documentation","text":""},{"location":"develop/documentation/#installation","title":"Installation","text":"<p>To install documentation dependencies:</p> <pre><code>pip install -e '.[doc]'\n</code></pre>"},{"location":"develop/documentation/#preview-locally","title":"Preview locally","text":"<p>To serve the documentation locally (for preview):</p> <pre><code>cd docs\n# make sure you are at PROJECT_ROOT/docs\n\nmike serve\n</code></pre> <p>or, you can use mkdocs to live-reload:</p> <pre><code>mkdocs serve\n</code></pre> <p>To check list of documentation versions:</p> <pre><code>make list\n</code></pre> <p>Check other utilities in <code>PROJECT_ROOT/docs/Makefile</code>.</p>"},{"location":"develop/documentation/#how-to-add-a-new-document","title":"How to add a new document","text":"<p>Steps:</p> <ol> <li>Create a new markdown file under <code>PROJECT_ROOT/docs/docs/</code> (e.g. <code>docs/docs/develop/foo.md</code>)</li> <li>Add an entry in <code>docs/mkdocs-nav.yml</code></li> </ol>"},{"location":"develop/restful-api/","title":"RESTful API","text":"<p>See implementation in <code>endpoints.py</code>.</p>"},{"location":"guide/advanced/","title":"Advanced Features","text":""},{"location":"guide/advanced/#plugins","title":"Plugins","text":""},{"location":"guide/advanced/#cli-plugins","title":"CLI plugins","text":"<p>CLI plugins are particularly useful if you want to pack up your workflow and share it with others.</p> <p>Demo plugin</p> <p>There is a demo plugin at <code>/PROJECT_ROOT/plugins/labtasker_plugin_task_count</code>.</p> <p>It creates a new custom command <code>labtasker task count</code>, which shows how many tasks are at each state.</p> <p></p> <p>To install officially bundled plugins:</p> PyPIGitHub <pre><code>pip install 'labtasker[plugins]'\n</code></pre> <pre><code>pip install 'labtasker[plugins] @ git+https://github.com/luocfprime/labtasker.git'\n</code></pre> <p>To install other plugins, simply install it like a regular Python package.</p> <pre><code>pip install labtasker-plugin-task-count\n</code></pre> <p>Note</p> <p>Behind the hood, it uses Typer command registry and setuptools entry points to implement custom CLI commands.</p> <p>To write your own CLI plugin, see Setuptools Doc and Typer Doc for details.</p>"},{"location":"guide/advanced/#workflow-plugins-wip","title":"Workflow plugins [WIP]","text":""},{"location":"guide/advanced/#custom-resolvers","title":"Custom resolvers","text":"<p>Sometimes after we fetched task args from the server, we need to convert it into other types (such as dataclasses) for further processing.</p> <p>Suppose you have a set of tasks submitted like this:</p> demo/advanced/custom_resolver/submit_job.py<pre><code>import labtasker\n\nif __name__ == \"__main__\":\n    for i in range(5):\n        print(f\"Submitting i={i}\")\n        labtasker.submit_task(\n            args={\n                \"args_a\": {\"a\": i, \"b\": \"boy\"},\n                \"args_b\": {\"foo\": 2 * i, \"bar\": \"baz\"},\n            }\n        )\n</code></pre> <p>You can manually specify the <code>required_fields</code> and convert them into your own dataclass manually:</p> demo/advanced/custom_resolver/wo.py<pre><code>import time\nfrom dataclasses import dataclass\n\nimport labtasker\n\n\n@dataclass\nclass ArgsGroupA:\n    a: int\n    b: str\n\n\n@dataclass\nclass ArgsGroupB:\n    foo: int\n    bar: str\n\n\n@labtasker.loop(required_fields=[\"args_a\", \"args_b\"], pass_args_dict=True)\ndef main(args):\n    args_a = ArgsGroupA(**args[\"args_a\"])\n    args_b = ArgsGroupB(**args[\"args_b\"])\n    print(f\"got args_a: {args_a}\")\n    print(f\"got args_b: {args_b}\")\n    time.sleep(0.5)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Now, you can achieve a more elegant solution by using a custom resolver:</p> demo/advanced/custom_resolver/w.py<pre><code>import time\nfrom dataclasses import dataclass\nfrom typing import Any, Dict\n\nfrom typing_extensions import Annotated\n\nimport labtasker\nfrom labtasker import Required\n\n\n@dataclass\nclass ArgsGroupA:\n    a: int\n    b: str\n\n\n@dataclass\nclass ArgsGroupB:\n    foo: int\n    bar: str\n\n\n@labtasker.loop()\ndef main(\n    # use type annotation/default values to automatically resolve the required_fields\n    # use the self-defined resolver to convert the task args into custom types\n    args_a: Annotated[\n        Dict[str, Any], Required(resolver=lambda a: ArgsGroupA(**a))\n    ],  # option1. use Annotated\n    args_b=Required(resolver=lambda b: ArgsGroupB(**b)),  # option2. use default kwarg\n):\n    print(f\"got args_a: {args_a}\")\n    print(f\"got args_b: {args_b}\")\n    time.sleep(0.5)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guide/advanced/#event-system","title":"Event system","text":"<p>Labtasker implements a simple event notification system based on Server Sent Events (SSE). This is particularly useful for real-time notifications, workflows, and other use cases.</p>"},{"location":"guide/advanced/#demo-labtasker-event-listen","title":"Demo: <code>labtasker event listen</code>","text":"<p>We use the <code>labtasker event listen</code> command as a demo.</p> <p>It will listen to the FSM state transition events from the server and print them out.</p> <p>labtasker event listen</p> <p></p>"},{"location":"guide/advanced/#demo-email-notification-on-task-failure","title":"Demo: email notification on task failure","text":"<p>Using the event listener, it is very easy to implement a simple email notification system on various events.</p> <p>For example, you can listen for <code>pending -&gt; failed</code> state transition events and send notification email.</p> demo/advanced/event_system/email_on_task_failure.py<pre><code>import subprocess\n\nimport typer\n\nfrom labtasker import connect_events\n\napp = typer.Typer()\n\n\ndef send(\n    recipient: str,\n    subject: str,\n    content: str,\n):\n    try:\n        mail_cmd = [\"mail\", \"-s\", subject, recipient]\n\n        process = subprocess.run(\n            mail_cmd,\n            input=content.encode(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n\n        if process.returncode == 0:\n            print(\"Email sent successfully!\")\n        else:\n            error_message = process.stderr.decode().strip()\n            print(f\"Error sending email: {error_message}\")\n            raise typer.Exit(process.returncode)\n    except Exception as e:\n        print(f\"Error sending email: {str(e)}\")\n        raise typer.Exit(1)\n\n\n@app.command()\ndef email_on_task_failure(\n    recipient: str = typer.Option(\n        ...,\n        envvar=\"RECIPIENT\",\n        help=\"Recipient email address (defaults to RECIPIENT environment variable)\",\n    ),\n):\n    \"\"\"\n    Send an email if received task failed event.\n\n    The recipient defaults to the RECIPIENT environment variable if not specified.\n    \"\"\"\n    listener = connect_events()\n    print(f\"Connected. Client listener ID: {listener.get_client_id()}\")\n    for event_resp in listener.iter_events():\n        if not event_resp.event.type == \"state_transition\":\n            continue\n\n        fsm_event = event_resp.event\n        if fsm_event.old_state == \"running\" and fsm_event.new_state == \"failed\":\n            # running -&gt; failed\n            print(f\"Task {fsm_event.entity_id} failed. Attempt to send email...\")\n            send(\n                recipient=recipient,\n                subject=\"Task failed\",\n                content=f\"Task {fsm_event.entity_id} failed.\",\n            )\n\n\nif __name__ == \"__main__\":\n    app()\n</code></pre> <p>Below is a recorded demo running a simulated unstable job with 50% chance of crashing.</p> demo/advanced/event_system/sim_unstable_job.py<pre><code>\"\"\"\nSimulate running an unstable job.\n\"\"\"\n\nimport random\nimport time\n\nimport labtasker\nfrom labtasker import Required\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(1.5)  # simulate a long-running job\n    if random.uniform(0, 1) &lt; 0.5:  # simulate unstable events\n        raise Exception(\"Random exception\")\n    return arg1 + arg2\n\n\n@labtasker.loop()\ndef main(arg1: int = Required(), arg2: int = Required()):\n    result = job(arg1, arg2)\n    print(f\"The result is {result}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>The notification script successfully captures the event and sends email.</p> <p>Email notification on task failure</p> <p></p>"},{"location":"guide/basic/","title":"Tutorial: Basic Workflow","text":"<p>Tip</p> <p>The code for this page is available on GitHub.</p> <p>Labtasker supports 2 sets of client APIs:</p> <ul> <li>Python Demo: Modify your Python Demo code with a few lines of changes to support Labtasker.</li> <li>Bash Demo: No modification to your Python Demo code is required. Simply wrap your command with <code>labtasker loop ...</code>.</li> </ul> <p>demo step by step</p> <p>This is a step-by-step demo of the basic workflow described in this page.</p> <p>Note: During the port configuration step, you can specify your own port.</p> <p></p>"},{"location":"guide/basic/#prerequisites","title":"Prerequisites","text":"<p>Make sure you have a deployed server.</p> <p>You can follow the Deployment guide to easily deploy a server.</p> <p>Make sure you have installed client tools.</p> <p>Following Installation.</p> <p>Make sure you have configured client.</p> <pre><code>labtasker init\n</code></pre> <p>It will guide you step-by-step:</p> <p>See more details about creating a queue in Queue Manual#create-queue.</p>"},{"location":"guide/basic/#step-1-submit-job-arguments-via-python-demo-or-cli-tool","title":"Step 1. Submit job arguments via Python Demo or CLI tool","text":"Bash DemoPython Demo demo/basic/bash_demo/submit_job.sh<pre><code>#!/bin/bash\n\n# This script submits jobs with different combinations of arg1 and arg2.\n\n# Loop through arg1 and arg2 values\nfor arg1 in {0..2}; do\n    for arg2 in {3..5}; do\n        echo \"Submitting with arg1=$arg1, arg2=$arg2\"\n        labtasker task submit --args '{\"arg1\": '$arg1', \"arg2\": '$arg2'}'\n        # Also a simpler equivalent fashion using -- as delimiter\n        # labtasker task submit -- --arg1 $arg1 --arg2 $arg2\n    done\ndone\n</code></pre> demo/basic/python_demo/submit_job.py<pre><code>import labtasker\n\nif __name__ == \"__main__\":\n    for arg1 in range(3):\n        for arg2 in range(3, 6):\n            print(f\"Submitting with arg1={arg1}, arg2={arg2}\")\n            labtasker.submit_task(\n                args={\"arg1\": arg1, \"arg2\": arg2},\n            )\n</code></pre> <p>See more details in Task Manual#submit-tasks.</p>"},{"location":"guide/basic/#step-2-run-job","title":"Step 2. Run job","text":"Bash DemoPython Demo demo/basic/bash_demo/run_job.sh<pre><code>#!/bin/bash\n\n# This script run jobs in loop by calling python job_main.py via labtasker loop\n# The argument can be automatically injected into the command line via %(...) syntax\nlabtasker loop -c 'python demo/basic/bash_demo/job_main.py --arg1 %(arg1) --arg2 %(arg2)'\n\n# Also a simpler fashion:\n# labtasker loop -- python demo/basic/bash_demo/job_main.py --arg1 '%(arg1)' --arg2 '%(arg2)'\n</code></pre> <p>where</p> demo/basic/bash_demo/job_main.py<pre><code>\"\"\"\nSuppose this is the given script to run certain job.\nYou would normally run this with ` python demo/bash_demo/job_main.py --arg1 1 --arg2 2`\n\"\"\"\n\nimport argparse\nimport time\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--arg1\", type=int)\n    parser.add_argument(\"--arg2\", type=int)\n\n    args = parser.parse_args()\n    result = job(args.arg1, args.arg2)\n    print(f\"The result is {result}\")\n</code></pre> demo/basic/python_demo/run_job.py<pre><code>import time\n\nimport labtasker\nfrom labtasker import Required\n\n\ndef job(arg1: int, arg2: int):\n    \"\"\"Simulate a long-running job\"\"\"\n    time.sleep(3)  # simulate a long-running job\n    return arg1 + arg2\n\n\n@labtasker.loop()\ndef main(arg1: int = Required(), arg2: int = Required()):\n    # The labtasker autofills the parameter specified by Required()\n    # or Annotated[Any, Required()]\n    # Alternatively, you can fill in the required fields\n    # in loop(required_fields=[\"arg1\", \"arg2\"]\n    # and access it via labtasker.task_info().args\n    result = job(arg1, arg2)\n    print(f\"The result is {result}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guide/basic/#check-pendingrunning-jobs","title":"Check pending/running jobs","text":"pendingrunning <pre><code>labtasker task ls -s pending\n</code></pre> <pre><code>labtasker task ls -s running\n</code></pre> <p>See more details in Loop Manual#create.</p>"},{"location":"guide/manual_loop/","title":"<code>labtasker loop</code>","text":"<p>Abstract</p> <p>This page details the loop related operations.</p> <p><code>labtasker loop</code> is the core command that runs the job automatically in a looped fashion. You can use it via Bash or Python.</p>"},{"location":"guide/manual_loop/#what-does-loop-do","title":"What does loop do","text":"<p>Labtasker loop automatically fetch tasks and run them.</p> <p>It does the following steps (mainly):</p> <ol> <li>Create worker if not exist or specified.    (keeping track of workers helps to prevent a single failing node from depleting all tasks in the queue by crashing    repeatedly)</li> <li>Call fetch_task API.</li> <li>Setup necessary context.</li> <li>Start a heartbeat thread.</li> <li>Run task.</li> <li>Submit summary (<code>success</code>/<code>failed</code>).</li> </ol>"},{"location":"guide/manual_loop/#usage","title":"Usage","text":""},{"location":"guide/manual_loop/#basic","title":"Basic","text":"Bash UsagePython Usage <p>For bash, you may use the <code>labtasker loop</code> command.</p> <p>See the example in the Basic Workflow.</p> <p>For python, you may use the <code>@labtasker.loop()</code> decorator.</p> <p>See the example in the Basic Workflow.</p> About the <code>required_fields</code>: the No more, No less principle <p>Labtasker fetches tasks based on specific requirements for the <code>args</code>. It follows a strict \"No More, No Less\" principle:</p> <ul> <li>No More: Tasks must not have extra fields that aren't needed.</li> <li>No Less: Tasks must include all required fields.</li> </ul>"},{"location":"guide/manual_loop/#why-is-this-rule-important","title":"Why is this rule important?","text":""},{"location":"guide/manual_loop/#case-1-the-no-more-rule","title":"Case 1: The \"No More\" Rule","text":"<p>Tasks should not include extra fields that aren't used.</p> <p>For example, consider a task in the queue:</p> <pre><code>task_id: 1\nargs:\n    prompt: \"an astronaut riding a horse\"\n    guidance_scale: 7.5\n</code></pre> <p>Now, let\u2019s say you have a script, <code>job.py</code>, with the following arguments:</p> <pre><code># job.py\nparser.add_argument(\"--prompt\", type=str, required=True)\nparser.add_argument(\"--guidance_scale\", type=float, default=100)\n</code></pre> <p>If you run <code>labtasker loop -- python job.py %(prompt)</code>, it fetches the task. However, since you didn\u2019t specify <code>guidance_scale</code> in the command, <code>job.py</code> will use the default value (<code>100</code>), even though the task\u2019s <code>args</code> says <code>guidance_scale: 7.5</code>. This mismatch can cause confusion because:</p> <ul> <li>The task\u2019s recorded <code>args</code> don\u2019t match what was actually used during execution.</li> <li>When you review the experimental records, extra fields in <code>args</code> might lead to incorrect assumptions.</li> </ul>"},{"location":"guide/manual_loop/#case-2-the-no-less-rule","title":"Case 2: The \"No Less\" Rule","text":"<p>Tasks must include all required fields.</p> <p>For instance, imagine you need to run this command:</p> <pre><code>labtasker loop -- python job.py %(prompt) %(guidance_scale)\n</code></pre> <p>But the task fetched looks like this:</p> <pre><code>task_id: 2\nargs:\n    prompt: \"an astronaut riding a horse\"\n</code></pre> <p>Here, the <code>guidance_scale</code> is missing from <code>args</code>, causing the command to fail because a required field is not provided. This is why all required fields must be present in the task's <code>args</code>.</p> <p>Therefore, the \"No More, No Less\" rule ensures tasks are fetched with exactly the right fields\u2014no extra or missing ones\u2014to avoid errors and inconsistencies.</p> <p>Can this be bypassed?</p> <p>What if you still want to fetch tasks with extra fields, even though it might have drawbacks? Is it possible?</p> <p>The answer is yes, and there\u2019s a simple workaround:</p> <pre><code>echo %(guidance_scale) &gt; /dev/null &amp;&amp; labtasker loop -- python job.py %(prompt)\n</code></pre> <p>This trick makes Labtasker think you\u2019ve used the extra field. However, it\u2019s always better to be explicit. If you want to ignore extra fields, make sure to do it intentionally.</p>"},{"location":"guide/manual_loop/#use-filter-to-get-only-the-task-you-want","title":"Use filter to get only the task you want","text":"<p>Similar to the <code>labtasker task ls</code> command, loop also supports filtering using MongoDB syntax queries to fetch only the task you want to run.</p> Bash UsagePython Usage <pre><code># example to execute task with tags \"experimental\" or \"diffusion\"\nlabtasker loop --extra-filter '{\"metadata.tags\": {\"$in\": [\"experimental\", \"diffusion\"]}}' -- python job.py %(prompt)\n</code></pre> <pre><code>@labtasker.loop(\n    required_fields=[\"prompt\"],\n    extra_filter={\"metadata.tags\": {\"$in\": [\"experimental\", \"diffusion\"]}}\n)\ndef main():\n    # your job code here\n</code></pre>"},{"location":"guide/manual_loop/#upon-task-failure","title":"Upon task failure","text":"<p>When a task fails, you will be presented with a 10-second countdown to choose one of the following options:</p> <ol> <li>Report: Mark the task as failed and submit the error message. The task state will be set to either \"pending\" or \"    failed,\" depending on the remaining retry attempts. Additionally, the worker's remaining attempts will be reduced by    one.</li> <li>Ignore: Disregard the failure, reset the task state to \"pending,\" restore the retry count as if no failure    occurred, and proceed to the next task.</li> </ol> <p>By default, the system selects the first option, which is ideal for background task execution, allowing Labtasker to automatically report failures.</p> <p>The second option is useful for debugging scenarios where you do not want to re-submit or manually adjust the task state. It enables you to isolate the failure without affecting subsequent tasks.</p>"},{"location":"guide/manual_queue/","title":"<code>labtasker queue</code>","text":"<p>Abstract</p> <p>This page details the queue related operations.</p> <p>Queues are the basic unit of task scheduling, worker management and authentication.</p>"},{"location":"guide/manual_queue/#create-queue","title":"Create queue","text":"<p>Before submitting a task, you need to create a queue for your tasks (this is analogous to registering an user account).</p> <p>It is recommended to use the <code>create-from-config</code> command to create a queue (after the config is properly setup via <code>labtasker config</code>).</p> <pre><code>labtasker queue create-from-config\n</code></pre> <p>or, you can create a queue different from the one specified in the config via <code>create</code>. See:</p> <pre><code>labtasker queue create --help\n</code></pre>"},{"location":"guide/manual_queue/#delete-queue","title":"Delete queue","text":"<p>It deletes the queue specified in the current configuration, note that with <code>--cascade</code> option, all associated tasks and workers will be deleted.</p> <pre><code>labtasker queue delete --help\n</code></pre>"},{"location":"guide/manual_queue/#update-queue","title":"Update queue","text":"<p>You may change the queue name, password and metadata via <code>update</code> command.</p> <p>If you do not want to specify password in the command, just run</p> <pre><code>labtasker queue update\n</code></pre> <p>and an interactive prompt will be shown.</p> <p>For more details, please run</p> <pre><code>labtasker queue update --help\n</code></pre>"},{"location":"guide/manual_queue/#get-queue-info","title":"Get queue info","text":"<p>To get current queue info, run</p> <pre><code>labtasker queue get\n</code></pre> <p>If you are trying to use a bash script, you can use the <code>--quiet</code> option to get the queue id only.</p> <pre><code>queue_id=$(labtasker queue get --quiet)\necho $queue_id\n# 30b5ef22-b45b-4f7a-ac48-d20360bbc04a\n</code></pre>"},{"location":"guide/manual_task/","title":"<code>labtasker task</code>","text":"<p>Abstract</p> <p>This page details the task related operations.</p>"},{"location":"guide/manual_task/#submit-tasks","title":"Submit tasks","text":""},{"location":"guide/manual_task/#specifying-task-args-via-args-option-or-args-argument","title":"Specifying task args via <code>--args</code> option or <code>[ARGS]</code> argument","text":"<p>You can submit the task args that you want to run.</p> Specify args via <code>--args</code> optionSpecify args via <code>[ARGS]</code> argument <pre><code>labtasker task submit --args '{\"foo.bar\": 0.1}'\n</code></pre> <p>Tip</p> <p>Specify task args via <code>[ARGS]</code> argument is convenient if you have a job script that takes CLI arguments directly.</p> <p>Normally, you would run it like this (suppose your job script takes <code>--foo.bar</code> as a CLI argument):</p> <pre><code>python train.py --foo.bar 0.1\n</code></pre> <p>You can directly copy and paste the <code>--foo.bar</code> argument as a positional argument to <code>labtasker task submit</code>.</p> <pre><code>labtasker task submit -- --foo.bar 0.1\n</code></pre> <p>arg cast to python primitive</p> <p>By default, <code>labtasker task submit</code> will cast the <code>args</code> to python primitive types.</p> <p>Therefore,  if you specify <code>-- --foo 0.1</code>, it will be cast to <code>float</code> type rather than remain as a string.</p> <p>The same principle applies to <code>labtasker task update</code>.</p> <p>Dict nesting</p> <p>Labtasker parses the dot-separated top-level keys as keys to nested dicts.</p> <p>For example:</p> <p><code>--foo.bar 0.1</code> would give you task <code>args</code> as <code>{\"foo\": {\"bar\": 0.1}}</code>.</p> <p>If you are accessing task <code>args</code> via <code>labtasker.task_info().args</code>, remember to access like <code>labtasker.task_info().args[\"foo\"][\"bar\"]</code> rather than <code>labtasker.task_info().args[\"foo.bar\"]</code>.</p> <p>The same principle applies to <code>args</code>, <code>metadata</code>, <code>summary</code> for queues, tasks and workers.</p>"},{"location":"guide/manual_task/#metadata","title":"Metadata","text":"<p>Metadata is handy if you want to filter tasks according to certain conditions.</p> <p>For example, you may implement a custom tag system to manage your tasks through metadata.</p> <p>Example</p> <pre><code># submit a task 1\nlabtasker task submit --metadata '{\"tags\": [\"test\", \"experimental\"]}' -- --foo.bar 0.1\n# submit a task 2\nlabtasker task submit --metadata '{\"tags\": [\"tag-baz\"]}' -- --foo.baz 0.2\n\n# loop up tasks that contains 'experimental' or 'tag-baz' (1)\nlabtasker task ls --extra-filter '{\"metadata.tags\": {\"$in\": [\"experimental\", \"tag-baz\"]}}' --quiet --no-pager\n\n# output:\n# dd4e3ce3-25a8-4176-bf4b-dba82187679d\n# 155f3872-1a07-45c9-85fb-51fce5cc29b5\n</code></pre> <ol> <li>See more about <code>labtasker task ls</code> in List tasks.</li> </ol>"},{"location":"guide/manual_task/#list-query-tasks","title":"List (query) tasks","text":"<p>By default, <code>labtasker task ls</code> displays all tasks in the queue. Output is shown through a pager like <code>less</code> by default. Add <code>--no-pager</code> to display directly in the terminal.</p> <p>You can filter tasks using:</p> <ul> <li><code>--task-id</code> or <code>--task-name</code> for basic filtering</li> <li><code>--extra-filter</code> for advanced queries</li> </ul>"},{"location":"guide/manual_task/#using-extra-filters","title":"Using Extra Filters","text":"<p>Choose between two filter syntaxes:</p> <ol> <li> <p>Python Native Syntax: Intuitive to use but less powerful.    <pre><code># Find tasks where args.foo.bar &gt; 0.1\nlabtasker task ls --extra-filter 'args.foo.bar &gt; 0.1' --quiet --no-pager\n</code></pre> Note: Does not support <code>not in</code>, <code>not expr</code>, or <code>!=</code> due to null value ambiguities</p> </li> <li> <p>MongoDB Syntax: More powerful but requires MongoDB knowledge.    <pre><code># Find tasks where args.foo.bar &gt; 0.1\nlabtasker task ls --extra-filter '{\"args.foo.bar\": {\"$gt\": 0.1}}' --quiet --no-pager\n</code></pre></p> </li> </ol> <p>You can see the transpiled query using <code>--verbose</code> option.</p>"},{"location":"guide/manual_task/#modify-update-tasks","title":"Modify (update) tasks","text":"<p>By default, <code>labtasker task update</code> will open terminal editor (such as vim) to allow you edit the task info.</p> <p>However, you may specify the <code>--update</code> option or the <code>[UPDATES]</code> argument to specify the fields to update instead of opening the editor.</p> Specify args via <code>--update</code>/<code>-u</code> optionSpecify args via <code>[UPDATES]</code> argument <pre><code># Example of updating task\nlabtasker task update --id 9ca765ce-94fe-4e2f-b88b-954b3412e607 -u 'args.arg1=0.3' -u 'args.arg2={\"arg21\": 0.4}'\n</code></pre> <pre><code># Example of updating task\nlabtasker task update --id 9ca765ce-94fe-4e2f-b88b-954b3412e607 -- args.arg1=0.3 args.arg2='{\"arg21\": 0.4}'\n</code></pre> <p>If you wish to use this command in a bash script, use <code>--quiet</code> option to disable unnecessary output and confirmations.</p>"},{"location":"guide/manual_task/#delete-tasks","title":"Delete tasks","text":"<pre><code>labtasker task delete --help\n</code></pre>"},{"location":"install/deployment/","title":"Deploy Server","text":"<p>TLDR</p> <p>The Labtasker server can be deployed in two ways, depending on the database backend and the chosen deployment method.</p> Deployment Method Server Database Python Native <code>labtasker-server</code> Local Environment A Python Emulated Embedded DB docker compose Run inside a container MongoDB Service"},{"location":"install/deployment/#method-1-python-native-easy","title":"Method 1. Python Native (Easy)","text":"<p>This is the simplest way to get started with Labtasker using only Python dependencies. The embedded database makes setup fast and straightforward.</p> <pre><code>pip install labtasker\n</code></pre> <p>Then, to start a Labtasker server (with embedded database) in the background, run the following command:</p> <pre><code>labtasker-server serve --host 0.0.0.0 --port 9321 &amp;\n</code></pre>"},{"location":"install/deployment/#method-2-docker-compose-advanced","title":"Method 2. Docker Compose (Advanced)","text":"<p>This method is recommended for scenarios where you need more robust database capabilities and containerized deployment.</p>"},{"location":"install/deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Compose</li> </ul>"},{"location":"install/deployment/#step-1-configuration","title":"Step 1: Configuration","text":"<ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/luocfprime/labtasker.git\ncd labtasker\n</code></pre></p> </li> <li> <p>Create your environment file:    <pre><code>cp server.example.env server.env\n</code></pre></p> </li> <li> <p>Configure your settings in <code>server.env</code>:</p> <ul> <li>Configure MongoDB.</li> <li>Configure server ports.</li> <li>Configure how often you want to check for task timeouts.</li> </ul> </li> </ol>"},{"location":"install/deployment/#step-2-start-services","title":"Step 2: Start services","text":"<ol> <li> <p>Start services (first time or update existing services):    <pre><code>docker compose --env-file server.env up -d --pull always\n</code></pre></p> </li> <li> <p>Check status:    <pre><code>docker compose --env-file server.env ps\n</code></pre></p> </li> <li> <p>View logs:    <pre><code>docker compose --env-file server.env logs -f\n</code></pre></p> </li> </ol>"},{"location":"install/deployment/#database-management","title":"Database Management","text":"<p>To expose MongoDB for external tools (this is potentially risky):</p> <ol> <li>Set <code>EXPOSE_DB=true</code> in <code>server.env</code></li> <li>Optionally set <code>DB_PORT</code> to change the exposed port (default: 27017)</li> <li>Use tools like MongoDB Compass to connect to the database.</li> </ol>"},{"location":"install/install/","title":"Installation","text":"<p>Installation</p> <p>To use Labtasker, you need to:</p> <ol> <li>Have a deployed server. See Deploy Server for more details.</li> <li>Install labtasker client tools via pip.</li> </ol> <p>The client tools is a Python package that you can install on your local machine.</p> <p>System Requirements</p> <p>Labtasker is unit tested on Windows, Linux and MacOS. Nevertheless, it is recommended to use a POSIX compliant system.</p> From PyPIFrom Github <pre><code>pip install labtasker\n</code></pre> <p>To install bundled plugins, run with optional dependency install:</p> <pre><code>pip install 'labtasker[plugins]'\n</code></pre> <pre><code>pip install git+https://github.com/luocfprime/labtasker.git\n</code></pre> <p>To install bundled plugins, run with optional dependency install:</p> <pre><code>pip install 'labtasker[plugins] @ git+https://github.com/luocfprime/labtasker.git'\n</code></pre>"}]}